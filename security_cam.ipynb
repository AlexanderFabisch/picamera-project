{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Do-it-yourself Security Camera"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Author: Alexander Fabisch <afabisch@googlemail.com>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code for this project is available at [github](https://github.com/AlexanderFabisch/picamera-project)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hardware"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Shopping list:\n",
      "\n",
      "* [Raspberry Pi, model B](http://www.raspberrypi.org/products/model-b/)\n",
      "* [Raspberry Pi camera module](http://www.exp-tech.de/raspberry-pi-camera-module)\n",
      "* 8 GB SD card\n",
      "* [WiFi USB stick](http://www.amazon.de/gp/product/B008IFXQFU/ref=oh_aui_detailpage_o01_s00?ie=UTF8&psc=1)\n",
      "* power cable (the same as for mobile phones)\n",
      "* [External battery](http://www.amazon.de/gp/product/B00H9BEC8E/ref=oh_aui_detailpage_o01_s00?ie=UTF8&psc=1)\n",
      "* Dummy surveillance camera (case for the Raspberry Pi, I took the idea from [this](http://www.instructables.com/id/Raspberry-Pi-as-low-cost-HD-surveillance-camera/) guide)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assembly:\n",
      "\n",
      "* Install [Raspbian](http://www.raspbian.org/RaspbianImages) OS on the SD card\n",
      "* Insert the SD card, insert the WiFi stick, connect the power cable\n",
      "* Set up the WiFi stick (this was a little bit complicated in my case, see e.g. [here](http://www.raspberrypi.org/forums/viewtopic.php?p=462982#p462982) and [here](http://www.howtogeek.com/167425/how-to-setup-wi-fi-on-your-raspberry-pi-via-the-command-line/) for some tips), to do this you need some kind of access to the system, either via ssh with a network cable or via keybord and monitor\n",
      "* [Set up the camera module](http://www.raspberrypi.org/help/camera-module-setup/)\n",
      "* Put the Raspberry Pi in the dummy surveillance camera\n",
      "\n",
      "To test the setup, you can try some of [these ideas](http://www.raspberrypi.org/learning/python-picamera-setup/), e.g. record a time-lapse video."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Software Component"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need a daemon that is running on the Raspberry Pi to capture and analyze the images and to upload the images to a server.\n",
      "\n",
      "TODO notify owner in case of alarm?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Activity Detection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%capture --no-display\n",
      "%pylab inline --no-import-all\n",
      "from desktop.image_processing import *\n",
      "from desktop.image_debugging import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I implemented a very specific solution for my security camera. My goal is to detect if the door is opened and if this is the case, I will upload the image to my server. This will usually not trigger when a person in the flat walks around and, hence, protects his or her privacy. To get an understanding of the captured scene, take a look at an image from my camera:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image = load_image(\"desktop/data/1418332576.jpg\")\n",
      "plt.figure(figsize=(4, 3))\n",
      "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
      "plt.imshow(image)\n",
      "_ = plt.setp(plt.gca(), xticks=(), yticks=())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be a little bit more flexible I measure the edges of the door in world coordinates, find a projection from world coordinates to image coordinates and compare the position of these projected edges with edges that we can actually find in the image. All of the required algorithms will usually be explained in a basic course about computer vision at a university. However, we don't have to implement these algorithms. We can find them in [scikit-image](http://scikit-image.org/)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Edge Detection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will start with a simple edge detection approach:\n",
      "\n",
      "* convert the image to grayscale\n",
      "* convolve the image with a [sobel filter](http://scikit-image.org/docs/0.10.x/api/skimage.filter.html#sobel) to detect edge pixels\n",
      "* apply [Otsu's method](http://scikit-image.org/docs/0.10.x/api/skimage.filter.html#skimage.filter.threshold_otsu) to binarize the edge image"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image_sobel, image_edges = detect_edges(image)\n",
      "\n",
      "plt.figure(figsize=(9, 3))\n",
      "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
      "plt.subplot(121)\n",
      "plt.imshow(image_sobel, cmap=plt.cm.gray)\n",
      "plt.setp(plt.gca(), xticks=(), yticks=())\n",
      "plt.subplot(122)\n",
      "plt.imshow(image_edges, cmap=plt.cm.gray)\n",
      "_ = plt.setp(plt.gca(), xticks=(), yticks=())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can filter the edges further: we can\n",
      "\n",
      "* use a hough transform to extract lines from edge pixels and\n",
      "* keep only those edge pixels that are near a detected line"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skimage.transform import hough_line, hough_line_peaks\n",
      "\n",
      "hough_accumulator, angles, dists = hough_line(image_edges)\n",
      "hspace, angles, dists = hough_line_peaks(\n",
      "    hough_accumulator, angles, dists, threshold=150.0)\n",
      "\n",
      "# Get edge pixels in vicinity of lines\n",
      "Pi_line_points = check_edge_is_on_line(image_edges, angles, dists)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rows, cols = image.shape[:2]\n",
      "\n",
      "image_edges_filtered = np.copy(image)\n",
      "draw_to_image(image_edges_filtered, Pi_line_points)\n",
      "\n",
      "plt.figure(figsize=(8, 6))\n",
      "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
      "plt.imshow(image_edges_filtered, cmap=plt.cm.gray)\n",
      "for l in line_points(cols, angles, dists):\n",
      "    plt.plot((l[0, 0], l[1, 0]), (l[0, 1], l[1, 1]), \"r\")\n",
      "plt.gca().axis((0, cols, rows, 0))\n",
      "_ = plt.setp(plt.gca(), xticks=(), yticks=())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Camera Transform"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To project points from the 3D world coordinate system to the 2D image coordinate system, we have to know the parameters of the [camera sensor](http://elinux.org/Rpi_Camera_Module#Technical_Parameters), the distortion, and the transformation from world frame to camera frame (`cam2world`). The camera parameters are the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "camera_params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other parameters can be found automatically. A method that is widely used is [Tsai's camera calibration method](http://www.vision.caltech.edu/bouguetj/calib_doc/papers/Tsai.pdf). I used a simplified approach here. I measured the translation of the camera in the world coordinate system and found the rotation and distortion model parameter with an optimization. The simple radial distortion model that I used requires to optimize only one parameter $\\kappa$. I picked 4 points in the world coordinate system and the corresponding pixels on the image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calibration points for camera parameters\n",
      "Pw_corners = np.array([[ 0.000, 0.0, 0, 1],\n",
      "                       [-0.100, 0.6, 0, 1],\n",
      "                       [-0.880, 0.6, 0, 1],\n",
      "                       [-1.315, 0.6, 0, 1],])\n",
      "Pi_corners = np.array([[420, 240],\n",
      "                       [372, 120],\n",
      "                       [194, 114],\n",
      "                       [81, 115]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The objective function of the optimization is the sum of squared errors between the pixels and the corresponding projected pixels. These kind of objective functions can be optimized very well by the [Levenberg-Marquardt algorithm](http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) which is implemented in `scipy.optimize.lstsq`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pytransform.camera import make_world_grid\n",
      "\n",
      "params = optimize_transform(camera_params, Pw_corners, Pi_corners)\n",
      "cam2world = transform_from(matrix_from_euler_xyz(params[:3]), params[3:6])\n",
      "kappa = params[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now plot the pixels of the calibration points on the image (blue), the points in the world projected to the image (green) and the estimated coordinate system (represented by a yellow grid in the x-y plane)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Transform points to image coordinates\n",
      "Pi_corners_proj = world2image(Pw_corners, cam2world, kappa=kappa, **camera_params)\n",
      "\n",
      "# Grid that we display for debugging purposes\n",
      "Pw_grid = make_world_grid(n_lines=16, n_points_per_line=101,\n",
      "                          xlim=(-1.5, 0.0), ylim=(-0.9, 0.6))\n",
      "Pi_grid = world2image(Pw_grid, cam2world, kappa=kappa, **camera_params)\n",
      "\n",
      "plt.figure(figsize=(8, 6))\n",
      "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
      "plt.setp(plt.gca(), xticks=(), yticks=())\n",
      "image_frames = np.copy(image)\n",
      "draw_to_image(image_frames, Pi_corners, color=[0, 0, 255], thick=True)\n",
      "draw_to_image(image_frames, Pi_grid, color=[255, 255, 0])\n",
      "draw_to_image(image_frames, Pi_corners_proj, thick=True)\n",
      "plt.imshow(image_frames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can plot the same points in the 3D in the world frame."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we can detect edge points and can transform points from the world coordinate system, we can check if the edges of the door correspond to detected edges on the image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pytransform.camera import make_world_line\n",
      "\n",
      "Pw_door_lo = make_world_line([0, 0, 0, 1], [0, -0.85, 0, 1], 51)\n",
      "Pw_door_hi = make_world_line([0, 0, 0, 1], [0, 0, 1, 1], 51)\n",
      "\n",
      "P_door_from_world_lo, matching_ratio_lo = check_line_is_edge(\n",
      "    Pw_door_lo, Pi_line_points, cam2world, kappa, camera_params)\n",
      "P_door_from_world_hi, matching_ratio_hi = check_line_is_edge(\n",
      "    Pw_door_hi, Pi_line_points, cam2world, kappa, camera_params)\n",
      "\n",
      "plt.figure(figsize=(8, 6))\n",
      "plt.setp(plt.gca(), xticks=(), yticks=())\n",
      "image_door = np.copy(image)\n",
      "draw_to_image(image_door, P_door_from_world_lo, thick=True)\n",
      "draw_to_image(image_door, P_door_from_world_hi, thick=True)\n",
      "_ = plt.imshow(image_door)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from pytransform.transformations import plot_transform\n",
      "\n",
      "plt.figure(figsize=(12, 12))\n",
      "ax = plt.subplot(111, projection=\"3d\", aspect=\"equal\")\n",
      "plot_transform(ax, A2B=cam2world)\n",
      "ax.scatter(Pw_grid[:, 0], Pw_grid[:, 1], Pw_grid[:, 2], s=1, c=\"g\")\n",
      "ax.scatter(Pw_door_lo[:, 0], Pw_door_lo[:, 1], Pw_door_lo[:, 2], c=\"g\")\n",
      "ax.scatter(Pw_door_hi[:, 0], Pw_door_hi[:, 1], Pw_door_hi[:, 2], c=\"g\")\n",
      "ax.set_xlim((-1.5, 1.0))\n",
      "ax.set_ylim((-2.3, 0.2))\n",
      "ax.set_zlim((-0.2, 2.3))\n",
      "ax.set_xlabel(\"X\")\n",
      "ax.set_ylabel(\"Y\")\n",
      "_ = ax.set_zlabel(\"Z\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can define a simple heuristic to detect whether the door is open or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Matching ratios: %.1f%%, %.1f%%\"\n",
      "      % (100 * matching_ratio_lo, 100 * matching_ratio_hi))\n",
      "\n",
      "closed_door = matching_ratio_lo > 0.7 or matching_ratio_hi > 0.7\n",
      "if closed_door:\n",
      "    print(\"The door is closed\")\n",
      "else:\n",
      "    print(\"The door is open\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the door is closed, we will immediately delete the captured image."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Commercial Solutions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Appendix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%load desktop/image_processing.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}